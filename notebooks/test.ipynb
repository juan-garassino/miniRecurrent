{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import get_file\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "1115394/1115394 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "FILE_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
    "\n",
    "FILE_NAME = 'shakespeare.txt'\n",
    "\n",
    "path_to_file = get_file('shakespeare.txt', FILE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first citizen:\n",
      "before we proceed any further, hear\n"
     ]
    }
   ],
   "source": [
    "print(text_lower[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of text is: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "print(f'Lenght of text is: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 65 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocabulary = sorted(set(text))\n",
    "\n",
    "print(f'There are {len(vocabulary)} unique characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 39 unique characters\n"
     ]
    }
   ],
   "source": [
    "text_lower = text.lower()\n",
    "\n",
    "vocabulary_lower = sorted(set(text_lower))\n",
    "\n",
    "print(f'There are {len(vocabulary_lower)} unique characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 41623 unique words\n"
     ]
    }
   ],
   "source": [
    "vocabulary_word = sorted(set(text_lower.split(' ')))\n",
    "\n",
    "print(f'There are {len(vocabulary_word)} unique words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_mapping = [(index, character) for index, character in enumerate(vocabulary)]\n",
    "\n",
    "word_mapping = [(index, word) for index, word in enumerate(vocabulary_word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_to_index = {character:index for index, character in enumerate(vocabulary)}\n",
    "\n",
    "word_to_index = {word:index for index, word in enumerate(vocabulary_word)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_character = {index:character for index, character in enumerate(vocabulary)}\n",
    "\n",
    "index_to_word = {index:word for index, word in enumerate(vocabulary_word)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from miniRecurrent.utils.tokenizer import tokenize, mapping, generate_training_data\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 26 unique characters\n",
      "There are 60 unique words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'algorithms': 1,\n",
       " 'and': 2,\n",
       " 'applications,': 3,\n",
       " 'are': 4,\n",
       " 'artificial': 5,\n",
       " 'as': 6,\n",
       " 'automatically': 7,\n",
       " 'based': 8,\n",
       " 'being': 9,\n",
       " 'build': 10,\n",
       " 'computer': 11,\n",
       " 'conventional': 12,\n",
       " 'data,': 13,\n",
       " 'decisions': 14,\n",
       " 'develop': 15,\n",
       " 'difficult': 16,\n",
       " 'do': 17,\n",
       " 'email': 18,\n",
       " 'experience.': 19,\n",
       " 'explicitly': 20,\n",
       " 'filtering': 21,\n",
       " 'improve': 22,\n",
       " 'in': 23,\n",
       " 'infeasible': 24,\n",
       " 'intelligence.': 25,\n",
       " 'is': 26,\n",
       " 'it': 27,\n",
       " 'known': 28,\n",
       " 'learning': 29,\n",
       " 'machine': 30,\n",
       " 'make': 31,\n",
       " 'mathematical': 32,\n",
       " 'model': 33,\n",
       " 'needed': 34,\n",
       " 'of': 35,\n",
       " 'on': 36,\n",
       " 'or': 37,\n",
       " 'order': 38,\n",
       " 'perform': 39,\n",
       " 'predictions': 40,\n",
       " 'programmed': 41,\n",
       " 'sample': 42,\n",
       " 'seen': 43,\n",
       " 'so.': 44,\n",
       " 'study': 45,\n",
       " 'subset': 46,\n",
       " 'such': 47,\n",
       " 'tasks.': 48,\n",
       " 'that': 49,\n",
       " 'the': 50,\n",
       " 'through': 51,\n",
       " 'to': 52,\n",
       " 'training': 53,\n",
       " 'used': 54,\n",
       " 'variety': 55,\n",
       " 'vision,': 56,\n",
       " 'where': 57,\n",
       " 'wide': 58,\n",
       " 'without': 59}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''Machine learning is the study of computer algorithms that \\\n",
    "improve automatically through experience. It is seen as a \\\n",
    "subset of artificial intelligence. Machine learning algorithms \\\n",
    "build a mathematical model based on sample data, known as \\\n",
    "training data, in order to make predictions or decisions without \\\n",
    "being explicitly programmed to do so. Machine learning algorithms \\\n",
    "are used in a wide variety of applications, such as email filtering \\\n",
    "and computer vision, where it is difficult or infeasible to develop \\\n",
    "conventional algorithms to perform the needed tasks.'''\n",
    "\n",
    "characters, vocabulary = tokenize(text)\n",
    "\n",
    "(character_to_index,\n",
    " index_to_character), (word_to_index,\n",
    "                       index_to_word) = mapping(characters, vocabulary)\n",
    " \n",
    "word_to_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_training_data(vocabulary, word_to_index, 2, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(234, 60)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(234, 60)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit ('lewagon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec6e4ea333e2292965c523f264f3f3438504971476de16e6adc68da302a506ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
