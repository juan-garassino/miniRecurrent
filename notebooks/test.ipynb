{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import get_file\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "1115394/1115394 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "FILE_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
    "\n",
    "FILE_NAME = 'shakespeare.txt'\n",
    "\n",
    "path_to_file = get_file('shakespeare.txt', FILE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first citizen:\n",
      "before we proceed any further, hear\n"
     ]
    }
   ],
   "source": [
    "print(text_lower[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of text is: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "print(f'Lenght of text is: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 65 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocabulary = sorted(set(text))\n",
    "\n",
    "print(f'There are {len(vocabulary)} unique characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 39 unique characters\n"
     ]
    }
   ],
   "source": [
    "text_lower = text.lower()\n",
    "\n",
    "vocabulary_lower = sorted(set(text_lower))\n",
    "\n",
    "print(f'There are {len(vocabulary_lower)} unique characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 41623 unique words\n"
     ]
    }
   ],
   "source": [
    "vocabulary_word = sorted(set(text_lower.split(' ')))\n",
    "\n",
    "print(f'There are {len(vocabulary_word)} unique words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_mapping = [(index, character) for index, character in enumerate(vocabulary)]\n",
    "\n",
    "word_mapping = [(index, word) for index, word in enumerate(vocabulary_word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_to_index = {character:index for index, character in enumerate(vocabulary)}\n",
    "\n",
    "word_to_index = {word:index for index, word in enumerate(vocabulary_word)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_character = {index:character for index, character in enumerate(vocabulary)}\n",
    "\n",
    "index_to_word = {index:word for index, word in enumerate(vocabulary_word)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-24 15:30:31.916499: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-24 15:30:32.410645: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-24 15:30:32.410666: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-24 15:30:32.479456: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-24 15:30:33.813523: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-24 15:30:33.813669: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-24 15:30:33.813679: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from miniRecurrent.utils.utils import tokenize, mapping, generate_training_data\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 26 unique characters\n",
      "There are 60 unique words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'algorithms': 1,\n",
       " 'and': 2,\n",
       " 'applications,': 3,\n",
       " 'are': 4,\n",
       " 'artificial': 5,\n",
       " 'as': 6,\n",
       " 'automatically': 7,\n",
       " 'based': 8,\n",
       " 'being': 9,\n",
       " 'build': 10,\n",
       " 'computer': 11,\n",
       " 'conventional': 12,\n",
       " 'data,': 13,\n",
       " 'decisions': 14,\n",
       " 'develop': 15,\n",
       " 'difficult': 16,\n",
       " 'do': 17,\n",
       " 'email': 18,\n",
       " 'experience.': 19,\n",
       " 'explicitly': 20,\n",
       " 'filtering': 21,\n",
       " 'improve': 22,\n",
       " 'in': 23,\n",
       " 'infeasible': 24,\n",
       " 'intelligence.': 25,\n",
       " 'is': 26,\n",
       " 'it': 27,\n",
       " 'known': 28,\n",
       " 'learning': 29,\n",
       " 'machine': 30,\n",
       " 'make': 31,\n",
       " 'mathematical': 32,\n",
       " 'model': 33,\n",
       " 'needed': 34,\n",
       " 'of': 35,\n",
       " 'on': 36,\n",
       " 'or': 37,\n",
       " 'order': 38,\n",
       " 'perform': 39,\n",
       " 'predictions': 40,\n",
       " 'programmed': 41,\n",
       " 'sample': 42,\n",
       " 'seen': 43,\n",
       " 'so.': 44,\n",
       " 'study': 45,\n",
       " 'subset': 46,\n",
       " 'such': 47,\n",
       " 'tasks.': 48,\n",
       " 'that': 49,\n",
       " 'the': 50,\n",
       " 'through': 51,\n",
       " 'to': 52,\n",
       " 'training': 53,\n",
       " 'used': 54,\n",
       " 'variety': 55,\n",
       " 'vision,': 56,\n",
       " 'where': 57,\n",
       " 'wide': 58,\n",
       " 'without': 59}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''Machine learning is the study of computer algorithms that \\\n",
    "improve automatically through experience. It is seen as a \\\n",
    "subset of artificial intelligence. Machine learning algorithms \\\n",
    "build a mathematical model based on sample data, known as \\\n",
    "training data, in order to make predictions or decisions without \\\n",
    "being explicitly programmed to do so. Machine learning algorithms \\\n",
    "are used in a wide variety of applications, such as email filtering \\\n",
    "and computer vision, where it is difficult or infeasible to develop \\\n",
    "conventional algorithms to perform the needed tasks.'''\n",
    "\n",
    "characters, vocabulary = tokenize(text)\n",
    "\n",
    "(character_to_index,\n",
    " index_to_character), (word_to_index,\n",
    "                       index_to_word) = mapping(characters, vocabulary)\n",
    " \n",
    "word_to_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_training_data(vocabulary, word_to_index, 2, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(234, 60)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(234, 60)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-24 15:30:53.366154: E tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2022-10-24 15:30:53.366196: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (celcius): /proc/driver/nvidia/version does not exist\n",
      "2022-10-24 15:30:53.367067: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 8, 100)            6100      \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 20)                9680      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,801\n",
      "Trainable params: 15,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### Let's build the neural network now\n",
    "from tensorflow.keras import layers, Sequential\n",
    "\n",
    "# Size of your embedding space = size of the vector representing each word\n",
    "embedding_size = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    layers.Embedding(\n",
    "        input_dim=len(vocabulary) + 1,  # 16 +1 for the 0 padding\n",
    "        input_length=8,  # Max_sentence_length (optional, for model summary)\n",
    "        output_dim=embedding_size,  # 100\n",
    "        mask_zero=True,  # Built-in masking layer :)\n",
    "    ))\n",
    "\n",
    "model.add(layers.LSTM(20))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 16 different words in your corpus\n",
      "X_pad.shape (3, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2.,  3.,  4.,  6.,  0.,  0.,  0.],\n",
       "       [ 1.,  2.,  7.,  4.,  8.,  9., 10., 11.],\n",
       "       [12.,  3.,  5., 13., 14., 15.,  5., 16.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "### Let's create some mock data\n",
    "def get_mock_up_data():\n",
    "    sentence_1 = 'Deep learning is super easy'\n",
    "    sentence_2 = 'Deep learning was super bad and too long'\n",
    "    sentence_3 = 'This is the best lecture of the camp!'\n",
    "\n",
    "    X = [sentence_1, sentence_2, sentence_3]\n",
    "    y = np.array([1., 0., 0.])\n",
    "\n",
    "    ### Let's tokenize the vocabulary\n",
    "    tk = Tokenizer()\n",
    "    tk.fit_on_texts(X)\n",
    "    vocab_size = len(tk.word_index)\n",
    "    print(f'There are {vocab_size} different words in your corpus')\n",
    "    X_token = tk.texts_to_sequences(X)\n",
    "\n",
    "    ### Pad the inputs\n",
    "    X_pad = pad_sequences(X_token, dtype='float32', padding='post')\n",
    "\n",
    "    return X_pad, y, vocab_size\n",
    "\n",
    "\n",
    "X_pad, y, vocab_size = get_mock_up_data()\n",
    "print(\"X_pad.shape\", X_pad.shape)\n",
    "X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7c18103af0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_pad, y, epochs=5, batch_size=16, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\",\n",
       "       b'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Let's get some text first\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "train_data, test_data = tfds.load(name=\"imdb_reviews\",\n",
    "                                  split=[\"train\", \"test\"],\n",
    "                                  batch_size=-1,\n",
    "                                  as_supervised=True)\n",
    "\n",
    "train_sentences, train_labels = tfds.as_numpy(train_data)\n",
    "test_sentences, test_labels = tfds.as_numpy(test_data)\n",
    "\n",
    "# Let's check two sentences\n",
    "train_sentences[0:2]\n",
    "\n",
    "# We have to convert the sentences into list of words! The computer won't do it for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this',\n",
       "  'was',\n",
       "  'an',\n",
       "  'absolutely',\n",
       "  'terrible',\n",
       "  'movie',\n",
       "  \"don't\",\n",
       "  'be',\n",
       "  'lured',\n",
       "  'in',\n",
       "  'by',\n",
       "  'christopher',\n",
       "  'walken',\n",
       "  'or',\n",
       "  'michael',\n",
       "  'ironside',\n",
       "  'both',\n",
       "  'are',\n",
       "  'great',\n",
       "  'actors',\n",
       "  'but',\n",
       "  'this',\n",
       "  'must',\n",
       "  'simply',\n",
       "  'be',\n",
       "  'their',\n",
       "  'worst',\n",
       "  'role',\n",
       "  'in',\n",
       "  'history',\n",
       "  'even',\n",
       "  'their',\n",
       "  'great',\n",
       "  'acting',\n",
       "  'could',\n",
       "  'not',\n",
       "  'redeem',\n",
       "  'this',\n",
       "  \"movie's\",\n",
       "  'ridiculous',\n",
       "  'storyline',\n",
       "  'this',\n",
       "  'movie',\n",
       "  'is',\n",
       "  'an',\n",
       "  'early',\n",
       "  'nineties',\n",
       "  'us',\n",
       "  'propaganda',\n",
       "  'piece',\n",
       "  'the',\n",
       "  'most',\n",
       "  'pathetic',\n",
       "  'scenes',\n",
       "  'were',\n",
       "  'those',\n",
       "  'when',\n",
       "  'the',\n",
       "  'columbian',\n",
       "  'rebels',\n",
       "  'were',\n",
       "  'making',\n",
       "  'their',\n",
       "  'cases',\n",
       "  'for',\n",
       "  'revolutions',\n",
       "  'maria',\n",
       "  'conchita',\n",
       "  'alonso',\n",
       "  'appeared',\n",
       "  'phony',\n",
       "  'and',\n",
       "  'her',\n",
       "  'pseudo',\n",
       "  'love',\n",
       "  'affair',\n",
       "  'with',\n",
       "  'walken',\n",
       "  'was',\n",
       "  'nothing',\n",
       "  'but',\n",
       "  'a',\n",
       "  'pathetic',\n",
       "  'emotional',\n",
       "  'plug',\n",
       "  'in',\n",
       "  'a',\n",
       "  'movie',\n",
       "  'that',\n",
       "  'was',\n",
       "  'devoid',\n",
       "  'of',\n",
       "  'any',\n",
       "  'real',\n",
       "  'meaning',\n",
       "  'i',\n",
       "  'am',\n",
       "  'disappointed',\n",
       "  'that',\n",
       "  'there',\n",
       "  'are',\n",
       "  'movies',\n",
       "  'like',\n",
       "  'this',\n",
       "  'ruining',\n",
       "  \"actor's\",\n",
       "  'like',\n",
       "  'christopher',\n",
       "  \"walken's\",\n",
       "  'good',\n",
       "  'name',\n",
       "  'i',\n",
       "  'could',\n",
       "  'barely',\n",
       "  'sit',\n",
       "  'through',\n",
       "  'it'],\n",
       " ['i',\n",
       "  'have',\n",
       "  'been',\n",
       "  'known',\n",
       "  'to',\n",
       "  'fall',\n",
       "  'asleep',\n",
       "  'during',\n",
       "  'films',\n",
       "  'but',\n",
       "  'this',\n",
       "  'is',\n",
       "  'usually',\n",
       "  'due',\n",
       "  'to',\n",
       "  'a',\n",
       "  'combination',\n",
       "  'of',\n",
       "  'things',\n",
       "  'including',\n",
       "  'really',\n",
       "  'tired',\n",
       "  'being',\n",
       "  'warm',\n",
       "  'and',\n",
       "  'comfortable',\n",
       "  'on',\n",
       "  'the',\n",
       "  'sette',\n",
       "  'and',\n",
       "  'having',\n",
       "  'just',\n",
       "  'eaten',\n",
       "  'a',\n",
       "  'lot',\n",
       "  'however',\n",
       "  'on',\n",
       "  'this',\n",
       "  'occasion',\n",
       "  'i',\n",
       "  'fell',\n",
       "  'asleep',\n",
       "  'because',\n",
       "  'the',\n",
       "  'film',\n",
       "  'was',\n",
       "  'rubbish',\n",
       "  'the',\n",
       "  'plot',\n",
       "  'development',\n",
       "  'was',\n",
       "  'constant',\n",
       "  'constantly',\n",
       "  'slow',\n",
       "  'and',\n",
       "  'boring',\n",
       "  'things',\n",
       "  'seemed',\n",
       "  'to',\n",
       "  'happen',\n",
       "  'but',\n",
       "  'with',\n",
       "  'no',\n",
       "  'explanation',\n",
       "  'of',\n",
       "  'what',\n",
       "  'was',\n",
       "  'causing',\n",
       "  'them',\n",
       "  'or',\n",
       "  'why',\n",
       "  'i',\n",
       "  'admit',\n",
       "  'i',\n",
       "  'may',\n",
       "  'have',\n",
       "  'missed',\n",
       "  'part',\n",
       "  'of',\n",
       "  'the',\n",
       "  'film',\n",
       "  'but',\n",
       "  'i',\n",
       "  'watched',\n",
       "  'the',\n",
       "  'majority',\n",
       "  'of',\n",
       "  'it',\n",
       "  'and',\n",
       "  'everything',\n",
       "  'just',\n",
       "  'seemed',\n",
       "  'to',\n",
       "  'happen',\n",
       "  'of',\n",
       "  'its',\n",
       "  'own',\n",
       "  'accord',\n",
       "  'without',\n",
       "  'any',\n",
       "  'real',\n",
       "  'concern',\n",
       "  'for',\n",
       "  'anything',\n",
       "  'else',\n",
       "  'i',\n",
       "  'cant',\n",
       "  'recommend',\n",
       "  'this',\n",
       "  'film',\n",
       "  'at',\n",
       "  'all']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's convert the list of sentences to a list of lists of words with a Keras utility function\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "X_train = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in train_sentences]\n",
    "X_test = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in test_sentences]\n",
    "\n",
    "X_train[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# This line trains an entire embedding for the words in your train set\n",
    "word2vec = Word2Vec(sentences=X_train, vector_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.7717568 ,  0.20403269,  0.5158379 , -0.608577  ,  0.06565354,\n",
       "        0.781588  ,  0.19114216, -0.03613279, -0.9344269 , -0.8756611 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at the representation of any word\n",
    "\n",
    "word2vec.wv['hello']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', 0.9648734331130981),\n",
       " ('thing', 0.9174069166183472),\n",
       " ('it', 0.8989912867546082),\n",
       " ('sequel', 0.8947099447250366),\n",
       " ('comment', 0.8818086981773376),\n",
       " ('word', 0.8767004609107971),\n",
       " ('review', 0.875000536441803),\n",
       " ('still', 0.871314287185669),\n",
       " ('experience', 0.8697939515113831),\n",
       " ('definitely', 0.8675937652587891)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's look at the 10 closest words to `movie`\n",
    "\n",
    "word2vec.wv.most_similar('movie', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To control the size of the embedding space, you just have to set-up the `vector_size` keyword\n",
    "\n",
    "word2vec = Word2Vec(\n",
    "    sentences=X_train[:1000],\n",
    "    vector_size=50)  # We keep the training short by taking only 1000 sentences\n",
    "\n",
    "len(word2vec.wv['computer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Word2Vec learns a representation for words that are present more than `min_count` number of times\n",
    "# This is to prevent learning representations based on a few examples only\n",
    "\n",
    "word2vec = Word2Vec(sentences=X_train[:1000], vector_size=50, min_count=5)\n",
    "\n",
    "try:\n",
    "    len(word2vec.wv['columbian'])\n",
    "except:\n",
    "    print(\"word seen only less than 5 times, excluded from corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As mentioned ealier, Word2vec trains an internal Neural network whose goal is to predict a word in a corpus\n",
    "# based on the words around it. This part of the sentence is called the window.\n",
    "# Its size corresponds to the number of words around word W used to predict this word W\n",
    "\n",
    "word2vec = Word2Vec(sentences=X_train[:10000],\n",
    "                    vector_size=100,\n",
    "                    window=5,\n",
    "                    min_count=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of training it on your training set (especially if it is very small), you can directly\n",
    "# load a pretrained embedding\n",
    "\n",
    "import gensim.downloader\n",
    "\n",
    "print(list(gensim.downloader.info()['models'].keys()))\n",
    "\n",
    "model_wiki = gensim.downloader.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wiki.most_similar('movie', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(sentences=X_train[:10000],\n",
    "                    vector_size=30,\n",
    "                    window=2,\n",
    "                    min_count=10)\n",
    "\n",
    "v_queen = word2vec.wv['queen']\n",
    "v_king = word2vec.wv['king']\n",
    "v_man = word2vec.wv['man']\n",
    "\n",
    "v_result = v_queen - v_king + v_man\n",
    "\n",
    "word2vec.wv.similar_by_vector(v_result)\n",
    "\n",
    "# You just did arithmetic directly on words!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit ('lewagon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec6e4ea333e2292965c523f264f3f3438504971476de16e6adc68da302a506ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
